<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src=""></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/bow-and-arrow.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://yifeizhou02.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yifeizhou02.github.io/">Yifei Zhou</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://azanette.com/">Andrea Zanette</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://www.jiayipan.me/">Jiayi Pan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://aviralkumar2907.github.io/">Aviral Kumar</a><sup>2,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of California, Berkeley,</span>
            <span class="author-block"><sup>2</sup>Carneige Mellon University,</span>
            <span class="author-block"><sup>3</sup>Google Deepmind</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="xxx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="xxx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/YifeiZhou02/ArCHer/tree/master"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
<p>Large language models (LLMs) have the potential to tackle sequential decision-making problems due to their generalist 
  capabilities. Instead of optimizing "myopic" surrogate objectives such as human preferences within a single turn, in such problems, 
  we wish to directly optimize long-term objectives, such as user satisfaction over an entire dialogue with an LLM or delayed success metrics in web navigation. 
  Multi-turn reinforcement learning (RL) provides an appealing approach to directly optimize long-term objectives, 
  but how can we design effective and efficient multi-turn RL algorithms for LLMs? In this work, 
  we propose an algorithmic framework to multi-turn RL for LLMs that preserves the flexibility of 
  token-by-token RL used in single-turn RL problems, while still accommodating long horizons and 
  delayed rewards more effectively. Our framework, the <strong>A</strong>cto<strong>r</strong>-<strong>C</strong>ritic 
  Framework with a <strong>H</strong>i<strong>e</strong>rarchical Structu<strong>r</strong>e (<strong>ArCHer</strong>), combines a high-level 
  off-policy RL algorithm that trains a value function with a low-level RL algorithm that trains a token-by-token policy. While ArCHer 
  can be instantiated with multiple RL algorithms, a particularly convenient instantiation is to use temporal difference (TD) learning at 
  the high level and on-policy token-level policy gradient at the low level.</p>
          <p>
Empirically, we show that ArCHer significantly improves efficiency and performance of multi-turn LLM tasks, attaining sample efficiency boosts of about <strong>100x</strong> over prior on-policy methods and converging to a much better performance than other off-policy methods.
          </p>

        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">What is Multi-Turn RL for language?</h2>


        <div class="content has-text-justified">
          <p>
            Generalist LLMs have the potential of serving as language agents to navigate through the web, make use of external tools, interact with human, and etc. 
            In order to succeed in these problems, an LLM needs to make a <strong>sequence</strong> of intelligent decisions over multiple turns of interaction with an external environment.
          </p>
          <p>
            Despite these "multi-turn" use cases, most methods for eliciting goal-directed behavior from LLMs often rely on "myopic" objectives that either attempt to mimic successful demonstrations, 
            or otherwise optimize for single-turn human preferences (single-turn RLHF). Policies trained via single-turn approaches degrade in performance as the length of interaction increases. 
            To address this problem, we apply "multi-turn" RL to directly maximize the long-term 
            objective of interest (e.g., customer satisfaction at the end of a multi-turn conversation with an LLM assistant), which can be formulated as scalar rewards.
          </p>
        <img id="multi_turn_gif" src="./static/images/website_gif.gif" >
        </div>
        <br/>
      </div>
    </div>
    <!--/ Animation. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Why do we need a hierarchical structure for RL with LLM?</h2>


        <div class="content has-text-justified">
          <p>
            Prior works in RL for LLMs either run entirely at the token level or at the utterance (a sequence of tokens) level, but they face different challenges:
            <ul>
                <li>Token-level methods face the challenge of an extremely long horizon (number of tokens per round * number of interactions), leading to numerical instabilities and slow convergence</li>
                <li>Utterance-level methods face the challenge of an exponential action space (exponential in the number of tokens per utterance), resulting in  difficulty in optimizing over such large action space</li>
            </ul>
          </p>
          <p>
            Our ArCHer for RL with language models can enjoy the best of both worlds, 
            where an off-policy temporal difference learning method can train an utterance-level value function at the high level, 
            and any policy gradient algorithm can optimize the token generation at each turn of the interaction at the low level, treating the high-level value function as the terminal reward for that turn.
          </p>
          <p>
            This allows for sample reuse and faster convergence, while avoiding the need to perform Bellman backups over each individual token, as the high-level critic is trained 
            at a coarser time-scale given by utterances. It also directly inherits tuning implementations from any 
            existing token-level policy gradient algorithm that has been developed for single-turn RL with preferences, 
            but with the utterance-level value function inheriting the role of the reward model. This way we are able to obtain the best of both utterance-based and token-based, 
            and off-policy and on-policy approaches for training LLMs.
          </p>
        </div>
        <br/>
      </div>
    </div>
    <!--/ Animation. -->
  </div>
</section>
    <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="./static/images/archer_diagram.001.jpeg" >
      <h2 class="subtitle has-text-centered">
        <strong>ArCHer</strong>: <strong>A</strong>cto<strong>r</strong>-<strong>C</strong>ritic 
  Framework with a <strong>H</strong>i<strong>e</strong>rarchical Structu<strong>r</strong>e
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">How does ArCHer work in practice?</h2>


        <div class="content has-text-justified">
          <p>
            We evaluate the performance of ArCHer on a wide range of tasks:
          </p>
        </div>
        <img id="env_figure" src="./static/images/detective.jpg" >
        <br>
        <p>
          We present the sample complexity and final performance of ArCHer compared to state-of-the-art baselines:
        </p>
        <br>
        <img id="env_figure" src="./static/images/main_experiments_website.png" >
        <p>
          ArCHer achieves <strong>100x</strong> better sample complexity than state-of-the-art on-policy method PPO and 
          converges to a better performance than state-of-the-art off-policy methods. For more results and ablations,
          please refer to our paper!
        </p>
      </div>
    </div>
    <!--/ Animation. -->
  </div>
</section>
  

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>Coming</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://yifeizhou02.github.io/" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
